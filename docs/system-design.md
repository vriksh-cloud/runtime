# Vriksh Cloud – Self-Hosted Node CLI
## System Design (Runtime on Docker / Kubernetes)

---

## 1. Purpose

The Self-Hosted Node CLI is the local execution engine for Vriksh Labs:

- User installs an npm package globally.
- Runs `vriksh run lab.yml`.
- CLI reads the Lab YAML (v2.0 spec).
- CLI provisions providers and resources on Docker or Kubernetes.
- CLI prints endpoints, credentials, and instructions.
- CLI tears down everything cleanly and generates a report.
- No Appwrite, no SaaS control plane, no billing, no multi-tenancy.

---

## 2. End-to-End Flow

```text
User → vriksh CLI (npm) → Runtime Engine → Provider Plugins → Docker / Kubernetes
```

High-level steps:

1.  **User** executes `vriksh run lab.yml`.
2.  **Runtime Engine**:
    *   Loads and validates Lab Spec v2.0.
    *   Selects execution backend (Docker or Kubernetes).
    *   Discovers and loads required provider plugins.
    *   Provisions all resources.
    *   Runs init scripts and scoring.
    *   Teardowns on completion or error.
    *   Local state and logs stored under `~/.vriksh`.

---

## 3. Components

### 3.1 CLI (npm Package)

Global installation:

```bash
npm install -g @vriksh/cloud-runtime
```

Binary: `vriksh`

**Commands:**

| Command | Description |
| :--- | :--- |
| `vriksh run <lab.yml>` | Execute a lab end-to-end locally |
| `vriksh validate <lab.yml>` | Validate Lab YAML against v2 schema |
| `vriksh logs` | Show or tail logs for the last run |
| `vriksh teardown` | Destroy resources for the last (or specified) run |
| `vriksh init-provider gitlab` | Scaffold local provider configuration for GitLab |
| `vriksh kubectl -- <args>` | Passthrough wrapper for Kubernetes cluster operations |

**Responsibilities:**

*   Parse CLI args.
*   Resolve file paths.
*   Invoke Runtime Engine APIs:
    *   `Engine.loadSpec(path)`
    *   `Engine.execute(spec, options)`
    *   `Engine.teardown(runId)`
    *   `Engine.fetchLogs(runId)`

### 3.2 Runtime Engine

**Responsibilities:**

*   Load Lab YAML.
*   Validate against Lab Spec v2.0 JSON Schema.
*   Build internal topology model (providers, resources, tasks).
*   Select execution backend (Docker or K8s).
*   Orchestrate provider plugins.
*   Maintain finite state machine (FSM) for the lab lifecycle.
*   Persist state in local store.
*   Generate logs and scoring reports.

**State store:**

```text
~/.vriksh/state.sqlite
```

Contains tables such as:
*   `runs` (run id, status, timestamps, backend, lab id)
*   `providers` (provider instances per run)
*   `events` (FSM transitions, errors, info)
*   `artifacts` (paths to reports, logs, scoring outputs)

### 3.3 Provider Plugins

Providers are npm-installable or local plugins:

*   `@vriksh/provider-gitlab`
*   `@vriksh/provider-k8s`
*   `@vriksh/provider-dbt`
*   `@vriksh/provider-kafka`
*   `@vriksh/provider-postgres`
*   `@vriksh/provider-linux-sandbox`

Discovered from:

```text
~/.vriksh/providers/
node_modules/@vriksh/provider-*
```

**Provider folder structure:**

```text
providers/
  gitlab/
    index.js
    schema.json
    templates/
    scripts/
```

**Provider interface:**

```ts
interface Provider {
  init(config: ProviderConfig, context: RunContext): Promise<void>;
  postInit?(config: ProviderConfig, context: RunContext): Promise<void>;
  runCheck?(checkConfig: CheckConfig, params: CheckParams): Promise<CheckResult>;
  teardown(context: RunContext): Promise<void>;
}
```

**Responsibilities:**

*   **init**:
    *   Provision containers or K8s resources.
    *   Generate credentials and endpoints.
*   **postInit**:
    *   Optional extra configuration, seeding, repo creation, etc.
*   **runCheck**:
    *   Execute scoring/verification logic.
*   **teardown**:
    *   Cleanly destroy resources.

### 3.4 Execution Backends

#### 3.4.1 Docker Backend (Default)

**Requirements:**
*   Docker Desktop or Docker Engine.
*   Docker Compose v2 recommended.

**Responsibilities:**
*   Create an isolated Docker network per lab run.
*   Start containers for each provider (GitLab, dbt runner, Kafka, etc).
*   Assign dynamic ports and record them in state.
*   Wait for readiness probes (health checks).
*   Map host ports to printable URLs (e.g. `http://localhost:8923`).

**Example steps:**
1.  `docker network create vriksh-lab-<run-id>`
2.  `docker run ... --network vriksh-lab-<run-id> ...`
3.  Read logs and health endpoints until ready.

**On teardown:**
*   `docker rm -f` all containers for that run.
*   `docker network rm` the lab network.

#### 3.4.2 Kubernetes Backend (Optional)

**Requirements:**
*   `kubectl` installed.
*   Local (kind/k3d/minikube) or remote cluster.
*   Kubeconfig set.

**Responsibilities:**
*   Create a dedicated namespace per lab run.
*   Apply manifests generated by provider plugins.
*   Wait for deployments and stateful sets to become ready.
*   Expose services via:
    *   NodePort
    *   Ingress
    *   or `kubectl port-forward` instructions.

**Teardown:**
*   Delete namespace.
*   Confirm all resources are gone.

**Backend selection:**
*   Via CLI flag (`--backend docker|k8s`) or lab runtime profile in YAML.
*   Default is Docker if unspecified.

---

## 4. Lab Spec YAML v2.0 (Usage in Runtime)

The runtime engine expects a Lab YAML v2.0 file (e.g. `lab.yml`):

*   Parsed and validated at the start (`PARSING`, `VALIDATION` states).
*   Defines:
    *   Providers required.
    *   Resources (containers, services, databases).
    *   Initialization scripts.
    *   Scoring checks.
    *   Teardown behavior.
    *   Instructions to show to the user.

The engine:
*   Converts YAML into an internal model.
*   Derives which providers to load.
*   Passes per-provider config subsets to each plugin.

---

## 5. Lab Lifecycle (FSM)

The runtime engine maintains a strict finite state machine:

```text
PARSING
  → VALIDATION
  → PREPARE
  → PROVISION
  → INIT
  → READY
  → RUN
  → SCORING
  → TEARDOWN
  → COMPLETED
```

**Description of each state:**

| State | Description |
| :--- | :--- |
| PARSING | Load YAML into memory; basic syntax parsing |
| VALIDATION | JSON Schema + semantic checks |
| PREPARE | Select backend, check system requirements, resolve plugins |
| PROVISION | Providers create containers/namespaces/resources |
| INIT | Run init scripts, seed data, configure tools |
| READY | All endpoints and credentials available to the user |
| RUN | User actively uses the lab |
| SCORING | Run checks against environment, compute score |
| TEARDOWN | Destroy resources and clean up |
| COMPLETED | Final report generated; state marked final |

All transitions are:
*   Logged to `state.sqlite`.
*   Emitted to local logs under `~/.vriksh/logs/<run-id>.log`.

---

## 6. Directory Layouts

### 6.1 User Project Folder

```text
my-lab/
  lab.yml
  instructions/
  init/
  teardown/
  manifests/
  assets/
```

*   `lab.yml`: Lab Spec v2.0.
*   `instructions/`: Markdown/html notes shown to user.
*   `init/`: Shell/Python scripts for initialization.
*   `teardown/`: Custom cleanup scripts (if needed).
*   `manifests/`: Raw K8s manifests or additional config.
*   `assets/`: Datasets, templates, or starter project files.

### 6.2 CLI Runtime Folder

```text
~/.vriksh/
  state.sqlite
  logs/
  providers/
```

*   `state.sqlite`: All runs, providers, FSM state, scoring metadata.
*   `logs/`: Per-run logs (`<run-id>.log`).
*   `providers/`: Custom or locally-installed providers.

---

## 7. System Requirements

### 7.1 Minimum

| Component | Requirement |
| :--- | :--- |
| OS | macOS, Linux, Windows WSL |
| RAM | 8 GB |
| CPU | 4 cores |
| Disk | 20 GB free |
| Node.js | 18+ LTS |
| Docker | Required (for Docker backend) |

### 7.2 Recommended for Multi-Provider Labs

| Component | Recommendation |
| :--- | :--- |
| RAM | 16–32 GB |
| CPU | 8 cores |
| Disk | NVMe, 100+ GB free |
| K8s | Local k3d cluster with 3 nodes |

**Additional workload guidance:**
*   GitLab: minimum 4 GB RAM, 2 CPU.
*   Kafka: minimum 4 GB RAM.
*   dbt: Python environment or dbt Docker image.

---

## 8. Security Model

*   Dedicated Docker network per lab run.
*   Strict container isolation; no privileged containers by default.
*   Per-lab volumes and directories; no cross-lab sharing.
*   Auto-generated secrets (passwords, tokens, PATs).
*   Secrets exist only in runtime and are deleted on teardown.

**For Kubernetes:**
*   Namespaces used as environment boundary.
*   All in-namespace resources deleted at teardown.

---

## 9. Observability

**Logs:**
`~/.vriksh/logs/<session-id>.log`

**Report:**
`./vriksh-report.json` created in the lab folder at the end of the run.

Report contains:
*   Provisioning time.
*   Initialization time.
*   Scoring results (per-check details).
*   Failures with messages and stack traces.
*   Engine version and provider versions.

---

## 10. Example Execution Flow

User runs:

```bash
vriksh run lab.yml
```

Sample console output:

```text
> Validating spec...
> Backend: Docker
> Creating network vriksh-lab-123...
> Starting GitLab provider...
> Waiting for GitLab to become ready...
> Starting dbt provider...
> Lab READY

Open:
  GitLab: http://localhost:8923
  Username: root-abc12
  Password: ******

Press ENTER when you are done to run scoring...

> Running scoring checks...
> Score: 90/100
> Generating report: ./vriksh-report.json
> Tearing down containers and network...
> Lab COMPLETED
```

---

## 11. Comparison to Managed Service (Conceptual)

| Aspect | Managed Service | Self-Hosted CLI |
| :--- | :--- | :--- |
| Control Plane | Appwrite-based multi-tenant SaaS | Local SQLite + filesystem |
| Identity / Billing | Users, teams, credits, subscriptions | None (single-user, no billing) |
| Execution Backend | Kubernetes cluster(s) per region | Local Docker or Kubernetes |
| Multi-user | Yes | No (local only) |
| Internet Required | Yes | Only for pulling images (optional) |
Persistence	Session-scoped only	Session-scoped only

---

## 12. One-Sentence Summary

The Self-Hosted Vriksh Node CLI is a standalone, npm-installable runtime that reads a Vriksh Lab YAML v2.0 file, provisions real infrastructure locally using Docker or Kubernetes via provider plugins, orchestrates the full lab lifecycle through a deterministic FSM, and tears everything down cleanly without any external control plane.

---

## 13. Reference Examples

Real-world examples of Lab Spec v2.0 configurations can be found in the `examples/` directory of this repository:

*   **GitLab Branching Basics:** `examples/gitlab/gitlab-branching-basics.yaml`
*   **GitLab CI Basics:** `examples/gitlab/gitlab-ci-basics.yaml`

These files demonstrate:
*   `topology`: Configuring providers (e.g., GitLab).
*   `tasks` & `checks`: Defining user objectives and automated scoring logic.
*   `init` & `teardown`: Lifecycle scripts.

